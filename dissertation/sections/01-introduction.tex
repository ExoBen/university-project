\documentclass[../dissertation.tex]{subfiles}
\begin{document}

\chapter{Introduction}

\section{Context}

Rendering massive networks (thousands of nodes connected by edges) in a browser is a frequent problem encountered in visualisation software that has become more prominent as both companies and regular users want to visualise a growing amount of data. As the number of nodes increase, most visualisation systems suffer an exponential performance decrease as each node in the network interacts with every other nodes. Additionally, for each node that is added, it is likely that multiple edges will be added to the network causing an even greater decrease in performance. Challenges involve the limits in browser performance as well as the fact that rendering information in a clear and meaningful way for the user is so difficult. 

There are two main barriers to easily visualising massive amounts of data. The first is the quantity of data that needs to be visualised by a client, which requires a lot of bandwidth, processing power and RAM. This could not be expected from a standard user with a: 3GHz quad-core processor, 4GB of RAM, no dedicated GPU and 30Mbps internet connection. Secondly, once the data is finally rendered, the result is a complex mass of nodes and edges of which minimal useful information can be taken. 

\section{Aims}

The intent of this project is to analyse many different software packages and outline different approaches which could be taken to help minimise the clutter of the visualisations and increase the performance of the software package that would display these large networks. A network visualisation system would then designed, implemented and evaluated. Furthermore, this project was proposed by SAS \cite{sas} - a world leader in data visualisation - who were interested in ways to let users visualise massive amounts of information on their own machines through a web application. Hence, another goal was to be able to provide SAS with recommendations on how they could improve current systems based on performance evaluations.

A successful outcome of the project would be a system where, instead of data being transferred to the client directly from the server in order to be visualised, that data is analysed and modified server-side in real time, and then a reduced version of the data is sent. This could possibly be done by: removing unnecessary nodes or edges, node bundling or edge bundling, or by sending an image to the client. This would result in the data being far more useful to the user, enabling them to make informed decisions based on the network presented to them, as opposed to users not being able to gain useful information from the visualisation due to the complex and cluttered layout. Additionally, this would reduce the processing required to render a network, resulting in lower processing power requirements, loading times and bandwidth requirements.

\section{Achievements}

This project succeeded in many important ways:

\begin{itemize}
    \item \textbf{A thorough review of the field of network visualisation.} This included analysing the field of visualisation, and gradually got more specific, covering network visualisation, visualisation of massive networks and finally visualising massive networks in a browser. SAS also provide a strong argument for why this project is necessary. These are covered in Chapter \ref{sec:review-of-field}. Additionally, research was conducted looking into how software handles visualising massive networks, which is covered in Chapter \ref{sec:research}.
    \item \textbf{A systematic review of existing network visualisation software.} This involved analysing, comparing and contrasting a wide range of software. Components tested included: ease of use of the software, how the software performed under load, and what support it had for displaying visualisations of massive networks. This review can be found in Chapter \ref{sec:systematic-review}.
    \item \textbf{A system was designed, implemented and evaluated with the purpose of proving how reducing the size of networks can lead to both an increase in performance and the visualisability of the information.} The end result was a web wrapper that encapsulated the Tulip Python API, which could be called from a JavaScript library. This would remove many nodes and edges deemed unnecessary to understanding the network as a whole by certain algorithms. This resulted in massive performance increases and let the information be visualised far more easily. For a network with one thousand nodes, loading times were decreased by 99.997\% when both Node Pruning and Node Bundling based on number of edges was applied. This is covered in Chapter \ref{chap:design}, \ref{chap:impl} and \ref{chap:testing-eval}.
\end{itemize}

\end{document}